# User Testing Guide - Adaptive Assessment UX

**Version:** 1.0
**Date:** 2025-10-06
**Purpose:** Guide for conducting user testing of adaptive assessment UX components

---

## Overview

This guide provides instructions for conducting user testing of the Neurlyn adaptive assessment system's UX components, including:
- Confidence progress panels
- Stage transition animations
- Progress messages and notifications
- Skip notifications
- Question counter with stage display

---

## Test Objectives

### Primary Objectives:
1. Verify users understand confidence indicators
2. Assess if progress messages are helpful and not distracting
3. Evaluate clarity of stage transitions
4. Ensure no confusion about assessment progress

### Secondary Objectives:
1. Identify any UX friction points
2. Gather feedback on visual design
3. Assess mobile responsiveness
4. Measure user satisfaction

---

## Test Participants

**Target:**
- 3-5 participants
- Mix of demographics (age, gender, tech proficiency)
- Prior experience with online assessments helpful but not required

**Recruitment Criteria:**
- Comfortable using computers/mobile devices
- Willing to provide honest feedback
- Available for 30-45 minute session

---

## Test Environment

### Setup:
1. **Device:** Desktop, tablet, or mobile phone
2. **Browser:** Chrome, Firefox, or Safari (latest versions)
3. **URL:** [Your testing URL]
4. **Session:** Fresh assessment (not resumed)

### Pre-Test Checklist:
- [ ] Test environment is accessible
- [ ] Recording tools ready (screen recording + audio)
- [ ] Consent forms prepared
- [ ] Test script printed/accessible
- [ ] Observation sheet ready

---

## Test Script

### Introduction (5 minutes)

"Thank you for participating in this user test. Today you'll be trying out a new personality assessment system. I'm interested in your honest feedback about the experience.

There are no right or wrong answers - we're testing the interface, not you. Please think aloud as you go through the assessment, sharing your thoughts and reactions.

Do you have any questions before we begin?"

### Task 1: Start Assessment & Initial Questions (10 minutes)

**Instructions:**
"Please start the assessment and answer the first 10-15 questions. As you go, let me know what you notice about the interface."

**Observe:**
- [ ] User notices confidence panel
- [ ] User understands what confidence bars represent
- [ ] User comments on progress indicators
- [ ] User's pace (rushing, pausing, steady)

**Probing Questions:**
1. "What do you think the percentage numbers mean?"
2. "Do you find the progress bars helpful or distracting?"
3. "How confident are you that the system is measuring your personality?"

### Task 2: Stage Transition (5 minutes)

**Instructions:**
"Continue answering questions until you see something change on the screen."

**Observe:**
- [ ] User notices stage transition animation
- [ ] User understands they've progressed to a new stage
- [ ] User's reaction to the transition (positive, neutral, negative)

**Probing Questions:**
1. "What just happened?"
2. "Does this make sense to you?"
3. "Is this helpful information, or could it be skipped?"

### Task 3: Progress Messages (5 minutes)

**Instructions:**
"Keep answering questions and pay attention to any messages that appear."

**Observe:**
- [ ] User notices progress messages (top-right notifications)
- [ ] User reads the messages
- [ ] Messages don't disrupt flow

**Probing Questions:**
1. "Did you notice the messages appearing?"
2. "Were they helpful or distracting?"
3. "Would you prefer more, less, or different information?"

### Task 4: Skip Notifications (5 minutes)

**Instructions:**
"Answer several questions consistently (e.g., all 4s or 5s) to trigger skip logic."

**Observe:**
- [ ] Skip notification appears when confidence reaches 85%+
- [ ] User understands why questions are being skipped
- [ ] User accepts the skip or expresses concern

**Probing Questions:**
1. "What does 'skipping additional questions' mean to you?"
2. "Do you trust the system to skip questions, or would you prefer to answer them all?"
3. "Is the explanation clear?"

### Task 5: Question Counter (5 minutes)

**Instructions:**
"Take a look at the question counter. What information is it showing you?"

**Observe:**
- [ ] User can locate the question counter
- [ ] User understands current question number and total
- [ ] User notices stage label

**Probing Questions:**
1. "How many questions have you answered so far?"
2. "How many total questions are there?"
3. "What stage are you currently in?"

---

## Post-Test Questions (5-10 minutes)

### Overall Experience:
1. On a scale of 1-5, how clear was the assessment process?
2. Did you feel informed about your progress throughout?
3. Was there anything confusing or frustrating?
4. What did you like most about the experience?
5. What would you change or improve?

### Specific Components:

**Confidence Panels:**
- Were the confidence bars helpful?
- Did you understand what they measured?
- Would you prefer more or less detail?

**Stage Transitions:**
- Were stage transitions clear?
- Did they feel too frequent or just right?
- Were the stage names meaningful?

**Progress Messages:**
- Were messages helpful or distracting?
- Did you read all of them?
- Would you prefer different timing or position?

**Skip Notifications:**
- Did you trust the system to skip questions?
- Was the explanation sufficient?
- Would you want more control over what gets skipped?

### Mobile-Specific (if applicable):
- Were all elements readable on your screen size?
- Did you need to zoom or scroll excessively?
- Were buttons easy to tap?

---

## Observation Checklist

### Usability Issues:
- [ ] Difficulty locating information
- [ ] Confusion about progress
- [ ] Technical errors or glitches
- [ ] Slow page loads
- [ ] Unresponsive elements

### User Behaviors:
- [ ] Scrolling patterns
- [ ] Hover/click behavior
- [ ] Reading speed
- [ ] Skipping or ignoring elements
- [ ] Returning to previous screens

### Emotional Responses:
- [ ] Positive reactions (smiles, enthusiasm)
- [ ] Negative reactions (frustration, confusion)
- [ ] Neutral/focused
- [ ] Disengagement

---

## Data Collection

### Quantitative Metrics:
- [ ] Time to complete 70 questions
- [ ] Number of pauses or hesitations
- [ ] Error rate (if applicable)
- [ ] Stage reached

### Qualitative Data:
- [ ] Direct quotes about UX elements
- [ ] Suggestions for improvement
- [ ] Pain points identified
- [ ] Positive feedback

---

## Analysis

### Success Criteria:

**Confidence Indicators:**
- ✅ 80%+ of users understand what confidence bars represent
- ✅ 70%+ find them helpful
- ✅ Users can accurately describe their confidence level

**Stage Transitions:**
- ✅ 90%+ notice stage transitions
- ✅ 80%+ understand they indicate progress
- ✅ 70%+ find them helpful (not distracting)

**Progress Messages:**
- ✅ 80%+ notice messages
- ✅ 70%+ find them informative
- ✅ <20% find them distracting

**Overall:**
- ✅ Average satisfaction rating ≥ 4/5
- ✅ No critical usability issues
- ✅ Users feel informed throughout

---

## Reporting

### Report Structure:

1. **Executive Summary**
   - Number of participants
   - Key findings (3-5 bullets)
   - Recommendations (prioritized)

2. **Detailed Findings**
   - By component (confidence, stages, messages, etc.)
   - Quotes and observations
   - Quantitative data

3. **Issues Identified**
   - Severity (Critical, High, Medium, Low)
   - Impact on user experience
   - Frequency (# of users affected)

4. **Recommendations**
   - Immediate fixes (critical issues)
   - Short-term improvements
   - Long-term enhancements

5. **Appendices**
   - Raw data
   - Session recordings (links)
   - Participant demographics

---

## Follow-Up Actions

### After Testing:
1. Consolidate findings across all sessions
2. Prioritize issues by severity and frequency
3. Create tickets for fixes/improvements
4. Share report with stakeholders
5. Plan implementation timeline

### Iteration:
- Re-test after implementing major changes
- Track metrics over time (completion rate, time, satisfaction)
- Continuous feedback collection

---

## Tips for Facilitators

### Do's:
- ✅ Remain neutral and non-leading
- ✅ Encourage thinking aloud
- ✅ Take detailed notes
- ✅ Record sessions (with consent)
- ✅ Thank participants sincerely

### Don'ts:
- ❌ Explain how things "should" work
- ❌ Defend design decisions
- ❌ Rush participants
- ❌ Interrupt their flow
- ❌ Make participants feel tested

---

## Resources

- Screen recording software: [OBS, Loom, etc.]
- Consent form template: [Link]
- Observation sheet template: [Link]
- Report template: [Link]

---

## Contact

For questions about this testing guide, contact: [Your contact info]

---

**Status:** Ready for use
**Next Review:** After 5 test sessions completed
